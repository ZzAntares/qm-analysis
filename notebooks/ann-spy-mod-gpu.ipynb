{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting direction of stock price index movement using artificial neural networks\n",
    "### Replicación del estudio por Julio Gutiérrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the daily data for Istambul Stock Exchange National Index 100 couldn't be found, the study is replicated with the SP&5000 daily data from 2007 to 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>256.600006</td>\n",
       "      <td>256.829987</td>\n",
       "      <td>256.149994</td>\n",
       "      <td>256.559998</td>\n",
       "      <td>256.559998</td>\n",
       "      <td>66935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "      <td>256.179993</td>\n",
       "      <td>256.309998</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>103715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>255.990005</td>\n",
       "      <td>256.299988</td>\n",
       "      <td>255.479996</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>69798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "      <td>256.470001</td>\n",
       "      <td>257.890015</td>\n",
       "      <td>255.630005</td>\n",
       "      <td>257.709991</td>\n",
       "      <td>257.709991</td>\n",
       "      <td>85562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-30</th>\n",
       "      <td>256.470001</td>\n",
       "      <td>257.600006</td>\n",
       "      <td>256.410004</td>\n",
       "      <td>256.750000</td>\n",
       "      <td>256.750000</td>\n",
       "      <td>53199200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "Date                                                                     \n",
       "2017-10-24  256.600006  256.829987  256.149994  256.559998  256.559998   \n",
       "2017-10-25  256.179993  256.309998  254.000000  255.289993  255.289993   \n",
       "2017-10-26  255.990005  256.299988  255.479996  255.619995  255.619995   \n",
       "2017-10-27  256.470001  257.890015  255.630005  257.709991  257.709991   \n",
       "2017-10-30  256.470001  257.600006  256.410004  256.750000  256.750000   \n",
       "\n",
       "               volume  \n",
       "Date                   \n",
       "2017-10-24   66935900  \n",
       "2017-10-25  103715300  \n",
       "2017-10-26   69798000  \n",
       "2017-10-27   85562500  \n",
       "2017-10-30   53199200  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy = pd.read_csv('data/SPY10.csv', index_col='Date', parse_dates=True)\n",
    "spy.columns = ['open', 'high', 'low', 'close', 'adjclose', 'volume']\n",
    "spy.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direction of daily change in the S&P500 is categorized as `0` or `1`. If the S&P500 Index at time `t` is higher than that at time `t-1`, direction `t` is `1`. If the S&P500 Index at time `t` is lower than that at time `t-1`, direction `t` is `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>prior_close</th>\n",
       "      <th>direction</th>\n",
       "      <th>future_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-16</th>\n",
       "      <td>255.210007</td>\n",
       "      <td>255.509995</td>\n",
       "      <td>254.820007</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>38221700</td>\n",
       "      <td>254.949997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-17</th>\n",
       "      <td>255.229996</td>\n",
       "      <td>255.520004</td>\n",
       "      <td>254.979996</td>\n",
       "      <td>255.470001</td>\n",
       "      <td>255.470001</td>\n",
       "      <td>31561000</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-18</th>\n",
       "      <td>255.899994</td>\n",
       "      <td>255.949997</td>\n",
       "      <td>255.500000</td>\n",
       "      <td>255.720001</td>\n",
       "      <td>255.720001</td>\n",
       "      <td>40888300</td>\n",
       "      <td>255.470001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-19</th>\n",
       "      <td>254.830002</td>\n",
       "      <td>255.830002</td>\n",
       "      <td>254.350006</td>\n",
       "      <td>255.789993</td>\n",
       "      <td>255.789993</td>\n",
       "      <td>61903800</td>\n",
       "      <td>255.720001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-20</th>\n",
       "      <td>256.700012</td>\n",
       "      <td>257.140015</td>\n",
       "      <td>255.770004</td>\n",
       "      <td>257.109985</td>\n",
       "      <td>257.109985</td>\n",
       "      <td>89176400</td>\n",
       "      <td>255.789993</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "      <td>257.480011</td>\n",
       "      <td>257.510010</td>\n",
       "      <td>256.019989</td>\n",
       "      <td>256.109985</td>\n",
       "      <td>256.109985</td>\n",
       "      <td>63915300</td>\n",
       "      <td>257.109985</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>256.600006</td>\n",
       "      <td>256.829987</td>\n",
       "      <td>256.149994</td>\n",
       "      <td>256.559998</td>\n",
       "      <td>256.559998</td>\n",
       "      <td>66935900</td>\n",
       "      <td>256.109985</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "      <td>256.179993</td>\n",
       "      <td>256.309998</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>103715300</td>\n",
       "      <td>256.559998</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>255.990005</td>\n",
       "      <td>256.299988</td>\n",
       "      <td>255.479996</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>69798000</td>\n",
       "      <td>255.289993</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "      <td>256.470001</td>\n",
       "      <td>257.890015</td>\n",
       "      <td>255.630005</td>\n",
       "      <td>257.709991</td>\n",
       "      <td>257.709991</td>\n",
       "      <td>85562500</td>\n",
       "      <td>255.619995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "Date                                                                     \n",
       "2017-10-16  255.210007  255.509995  254.820007  255.289993  255.289993   \n",
       "2017-10-17  255.229996  255.520004  254.979996  255.470001  255.470001   \n",
       "2017-10-18  255.899994  255.949997  255.500000  255.720001  255.720001   \n",
       "2017-10-19  254.830002  255.830002  254.350006  255.789993  255.789993   \n",
       "2017-10-20  256.700012  257.140015  255.770004  257.109985  257.109985   \n",
       "2017-10-23  257.480011  257.510010  256.019989  256.109985  256.109985   \n",
       "2017-10-24  256.600006  256.829987  256.149994  256.559998  256.559998   \n",
       "2017-10-25  256.179993  256.309998  254.000000  255.289993  255.289993   \n",
       "2017-10-26  255.990005  256.299988  255.479996  255.619995  255.619995   \n",
       "2017-10-27  256.470001  257.890015  255.630005  257.709991  257.709991   \n",
       "\n",
       "               volume  prior_close  direction  future_direction  \n",
       "Date                                                             \n",
       "2017-10-16   38221700   254.949997          1                 1  \n",
       "2017-10-17   31561000   255.289993          1                 1  \n",
       "2017-10-18   40888300   255.470001          1                 1  \n",
       "2017-10-19   61903800   255.720001          1                 1  \n",
       "2017-10-20   89176400   255.789993          1                 0  \n",
       "2017-10-23   63915300   257.109985          0                 1  \n",
       "2017-10-24   66935900   256.109985          1                 0  \n",
       "2017-10-25  103715300   256.559998          0                 1  \n",
       "2017-10-26   69798000   255.289993          1                 1  \n",
       "2017-10-27   85562500   255.619995          1                 0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy['prior_close'] = spy.close.shift(1)\n",
    "spy['direction'] = np.where(spy['close'] > spy['prior_close'], 1, 0)\n",
    "spy['future_direction'] = spy.direction.shift(-1)\n",
    "spy = spy.iloc[:-1]\n",
    "spy['future_direction'] = spy.future_direction.apply(int)\n",
    "spy.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample size by year varies in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>prior_close</th>\n",
       "      <th>direction</th>\n",
       "      <th>future_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "      <td>250</td>\n",
       "      <td>251</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      open  high  low  close  adjclose  volume  prior_close  direction  \\\n",
       "Date                                                                     \n",
       "2007   251   251  251    251       251     251          250        251   \n",
       "2008   253   253  253    253       253     253          253        253   \n",
       "2009   252   252  252    252       252     252          252        252   \n",
       "2010   252   252  252    252       252     252          252        252   \n",
       "2011   252   252  252    252       252     252          252        252   \n",
       "2012   250   250  250    250       250     250          250        250   \n",
       "2013   252   252  252    252       252     252          252        252   \n",
       "2014   252   252  252    252       252     252          252        252   \n",
       "2015   252   252  252    252       252     252          252        252   \n",
       "2016   252   252  252    252       252     252          252        252   \n",
       "2017   208   208  208    208       208     208          208        208   \n",
       "\n",
       "      future_direction  \n",
       "Date                    \n",
       "2007               251  \n",
       "2008               253  \n",
       "2009               252  \n",
       "2010               252  \n",
       "2011               252  \n",
       "2012               250  \n",
       "2013               252  \n",
       "2014               252  \n",
       "2015               252  \n",
       "2016               252  \n",
       "2017               208  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_samples = spy.groupby(spy.index.year).count()\n",
    "year_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of the data set for training, testing and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take an 80% of the samples of each year to build a training set, a 10% of the samples of each year to build a testing set and finally another 10% to build the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_frac = 0.8\n",
    "testing_set_frac = 0.1\n",
    "validation_set_frac = 0.1\n",
    "\n",
    "training_set, testing_set, validation_set = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ith in range(len(year_samples)):\n",
    "    year = year_samples.index[ith]\n",
    "    set_size = year_samples.iloc[ith].close\n",
    "\n",
    "    training_size = int(set_size * training_set_frac)\n",
    "    testing_size = int(set_size * testing_set_frac)\n",
    "    validation_size = int(set_size * validation_set_frac)\n",
    "    \n",
    "    year_set = spy[spy.index.year == year]\n",
    "\n",
    "    training_set = training_set.append(year_set[:training_size])\n",
    "    testing_set = testing_set.append(year_set[training_size:training_size + testing_size + 1])\n",
    "    validation_set = validation_set.append(year_set[training_size + testing_size + 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set consists of the first 80% of samples of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>prior_close</th>\n",
       "      <th>direction</th>\n",
       "      <th>future_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      open  high  low  close  adjclose  volume  prior_close  direction  \\\n",
       "Date                                                                     \n",
       "2007   200   200  200    200       200     200          199        200   \n",
       "2008   202   202  202    202       202     202          202        202   \n",
       "2009   201   201  201    201       201     201          201        201   \n",
       "2010   201   201  201    201       201     201          201        201   \n",
       "2011   201   201  201    201       201     201          201        201   \n",
       "2012   200   200  200    200       200     200          200        200   \n",
       "2013   201   201  201    201       201     201          201        201   \n",
       "2014   201   201  201    201       201     201          201        201   \n",
       "2015   201   201  201    201       201     201          201        201   \n",
       "2016   201   201  201    201       201     201          201        201   \n",
       "2017   166   166  166    166       166     166          166        166   \n",
       "\n",
       "      future_direction  \n",
       "Date                    \n",
       "2007               200  \n",
       "2008               202  \n",
       "2009               201  \n",
       "2010               201  \n",
       "2011               201  \n",
       "2012               200  \n",
       "2013               201  \n",
       "2014               201  \n",
       "2015               201  \n",
       "2016               201  \n",
       "2017               166  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.groupby(training_set.index.year).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing set consists of 10% of samples of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>prior_close</th>\n",
       "      <th>direction</th>\n",
       "      <th>future_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      open  high  low  close  adjclose  volume  prior_close  direction  \\\n",
       "Date                                                                     \n",
       "2007    26    26   26     26        26      26           26         26   \n",
       "2008    26    26   26     26        26      26           26         26   \n",
       "2009    26    26   26     26        26      26           26         26   \n",
       "2010    26    26   26     26        26      26           26         26   \n",
       "2011    26    26   26     26        26      26           26         26   \n",
       "2012    26    26   26     26        26      26           26         26   \n",
       "2013    26    26   26     26        26      26           26         26   \n",
       "2014    26    26   26     26        26      26           26         26   \n",
       "2015    26    26   26     26        26      26           26         26   \n",
       "2016    26    26   26     26        26      26           26         26   \n",
       "2017    21    21   21     21        21      21           21         21   \n",
       "\n",
       "      future_direction  \n",
       "Date                    \n",
       "2007                26  \n",
       "2008                26  \n",
       "2009                26  \n",
       "2010                26  \n",
       "2011                26  \n",
       "2012                26  \n",
       "2013                26  \n",
       "2014                26  \n",
       "2015                26  \n",
       "2016                26  \n",
       "2017                21  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.groupby(testing_set.index.year).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the validation testing set consists of the last 10% of samples of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>prior_close</th>\n",
       "      <th>direction</th>\n",
       "      <th>future_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      open  high  low  close  adjclose  volume  prior_close  direction  \\\n",
       "Date                                                                     \n",
       "2007    25    25   25     25        25      25           25         25   \n",
       "2008    25    25   25     25        25      25           25         25   \n",
       "2009    25    25   25     25        25      25           25         25   \n",
       "2010    25    25   25     25        25      25           25         25   \n",
       "2011    25    25   25     25        25      25           25         25   \n",
       "2012    24    24   24     24        24      24           24         24   \n",
       "2013    25    25   25     25        25      25           25         25   \n",
       "2014    25    25   25     25        25      25           25         25   \n",
       "2015    25    25   25     25        25      25           25         25   \n",
       "2016    25    25   25     25        25      25           25         25   \n",
       "2017    21    21   21     21        21      21           21         21   \n",
       "\n",
       "      future_direction  \n",
       "Date                    \n",
       "2007                25  \n",
       "2008                25  \n",
       "2009                25  \n",
       "2010                25  \n",
       "2011                25  \n",
       "2012                24  \n",
       "2013                25  \n",
       "2014                25  \n",
       "2015                25  \n",
       "2016                25  \n",
       "2017                21  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.groupby(validation_set.index.year).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the indicators data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the network is computed data of 10 technical indicators:\n",
    "\n",
    "- SMA10\n",
    "- WMA10\n",
    "- Momentum\n",
    "- Stochastic K%\n",
    "- Stochastic D%\n",
    "- RSI\n",
    "- MACD\n",
    "- Larry William's R%\n",
    "- A/D Oscilator\n",
    "- CCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_ = spy.open.values\n",
    "high = spy.high.values\n",
    "low = spy.low.values\n",
    "close = spy.close.values\n",
    "volume = spy.close.values\n",
    "sample_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2017-10-23 00:00:00, 2017-10-24 00:00:00, 2017-10-25 00:00:00, 2017-10-26 00:00:00, 2017-10-27 00:00:00]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators = pd.DataFrame(index=spy.index)\n",
    "indicators.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['sma'] = talib.SMA(close, timeperiod=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['wma'] = talib.WMA(close, timeperiod=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['momentum'] = talib.MOM(close, timeperiod=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revisar configuración, este produce los dos valores %K y %D\n",
    "slowk, slowd = talib.STOCH(high, low, close, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "indicators['stochk'] = slowk\n",
    "indicators['stochd'] = slowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['rsi'] = talib.RSI(close, timeperiod=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([        nan,         nan,         nan, ...,  1.71919997,\n",
       "        1.61562553,  1.68278911])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macd, macdsignal, macdhist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "indicators['macd'] = macd\n",
    "indicators['macdsig'] = macdsignal\n",
    "indicators['macdhist'] = macdhist\n",
    "macd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['williamsr'] = talib.WILLR(high, low, close, timeperiod=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['ad'] = talib.AD(high, low, close, volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicators['cci'] = talib.CCI(high, low, close, timeperiod=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is necessary to merge the indicators columns in to the splitted sets (training, testing, validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sma</th>\n",
       "      <th>wma</th>\n",
       "      <th>momentum</th>\n",
       "      <th>stochk</th>\n",
       "      <th>stochd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdsig</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>williamsr</th>\n",
       "      <th>ad</th>\n",
       "      <th>cci</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-23</th>\n",
       "      <td>255.471995</td>\n",
       "      <td>255.801811</td>\n",
       "      <td>2.159988</td>\n",
       "      <td>81.539607</td>\n",
       "      <td>87.806840</td>\n",
       "      <td>69.398467</td>\n",
       "      <td>1.883330</td>\n",
       "      <td>1.843897</td>\n",
       "      <td>0.039434</td>\n",
       "      <td>-39.660608</td>\n",
       "      <td>56032.256499</td>\n",
       "      <td>138.654546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24</th>\n",
       "      <td>255.665996</td>\n",
       "      <td>255.999630</td>\n",
       "      <td>1.940003</td>\n",
       "      <td>74.851831</td>\n",
       "      <td>82.615664</td>\n",
       "      <td>71.863007</td>\n",
       "      <td>1.859992</td>\n",
       "      <td>1.847116</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>-29.780912</td>\n",
       "      <td>56085.083879</td>\n",
       "      <td>99.064502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25</th>\n",
       "      <td>255.692994</td>\n",
       "      <td>255.931266</td>\n",
       "      <td>0.269989</td>\n",
       "      <td>54.127889</td>\n",
       "      <td>70.173109</td>\n",
       "      <td>57.373812</td>\n",
       "      <td>1.719200</td>\n",
       "      <td>1.821533</td>\n",
       "      <td>-0.102333</td>\n",
       "      <td>-63.248167</td>\n",
       "      <td>56114.921668</td>\n",
       "      <td>-46.220243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26</th>\n",
       "      <td>255.790994</td>\n",
       "      <td>255.917993</td>\n",
       "      <td>0.979996</td>\n",
       "      <td>50.947257</td>\n",
       "      <td>59.975659</td>\n",
       "      <td>59.718643</td>\n",
       "      <td>1.615626</td>\n",
       "      <td>1.780351</td>\n",
       "      <td>-0.164726</td>\n",
       "      <td>-53.846428</td>\n",
       "      <td>55946.586778</td>\n",
       "      <td>9.343250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27</th>\n",
       "      <td>256.066993</td>\n",
       "      <td>256.266902</td>\n",
       "      <td>2.759994</td>\n",
       "      <td>59.425852</td>\n",
       "      <td>54.833666</td>\n",
       "      <td>70.959988</td>\n",
       "      <td>1.682789</td>\n",
       "      <td>1.760839</td>\n",
       "      <td>-0.078050</td>\n",
       "      <td>-4.627848</td>\n",
       "      <td>56163.240328</td>\n",
       "      <td>124.155080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sma         wma  momentum     stochk     stochd        rsi  \\\n",
       "Date                                                                            \n",
       "2017-10-23  255.471995  255.801811  2.159988  81.539607  87.806840  69.398467   \n",
       "2017-10-24  255.665996  255.999630  1.940003  74.851831  82.615664  71.863007   \n",
       "2017-10-25  255.692994  255.931266  0.269989  54.127889  70.173109  57.373812   \n",
       "2017-10-26  255.790994  255.917993  0.979996  50.947257  59.975659  59.718643   \n",
       "2017-10-27  256.066993  256.266902  2.759994  59.425852  54.833666  70.959988   \n",
       "\n",
       "                macd   macdsig  macdhist  williamsr            ad         cci  \n",
       "Date                                                                           \n",
       "2017-10-23  1.883330  1.843897  0.039434 -39.660608  56032.256499  138.654546  \n",
       "2017-10-24  1.859992  1.847116  0.012877 -29.780912  56085.083879   99.064502  \n",
       "2017-10-25  1.719200  1.821533 -0.102333 -63.248167  56114.921668  -46.220243  \n",
       "2017-10-26  1.615626  1.780351 -0.164726 -53.846428  55946.586778    9.343250  \n",
       "2017-10-27  1.682789  1.760839 -0.078050  -4.627848  56163.240328  124.155080  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = training_set.join(indicators)\n",
    "testing_set = testing_set.join(indicators)\n",
    "validation_set = validation_set.join(indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>prior_close</th>\n",
       "      <th>direction</th>\n",
       "      <th>future_direction</th>\n",
       "      <th>sma</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum</th>\n",
       "      <th>stochk</th>\n",
       "      <th>stochd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macdsig</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>williamsr</th>\n",
       "      <th>ad</th>\n",
       "      <th>cci</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-02-21</th>\n",
       "      <td>145.610001</td>\n",
       "      <td>146.070007</td>\n",
       "      <td>145.350006</td>\n",
       "      <td>145.979996</td>\n",
       "      <td>116.886948</td>\n",
       "      <td>63971600</td>\n",
       "      <td>146.039993</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.144000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089997</td>\n",
       "      <td>89.992783</td>\n",
       "      <td>91.767628</td>\n",
       "      <td>67.286692</td>\n",
       "      <td>1.023571</td>\n",
       "      <td>0.914752</td>\n",
       "      <td>0.108819</td>\n",
       "      <td>-7.309015</td>\n",
       "      <td>1437.598145</td>\n",
       "      <td>81.889256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-22</th>\n",
       "      <td>146.050003</td>\n",
       "      <td>146.419998</td>\n",
       "      <td>145.169998</td>\n",
       "      <td>145.869995</td>\n",
       "      <td>116.798935</td>\n",
       "      <td>79067400</td>\n",
       "      <td>145.979996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.209999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659988</td>\n",
       "      <td>79.738918</td>\n",
       "      <td>87.692436</td>\n",
       "      <td>65.530479</td>\n",
       "      <td>1.014854</td>\n",
       "      <td>0.934773</td>\n",
       "      <td>0.080082</td>\n",
       "      <td>-17.027978</td>\n",
       "      <td>1455.101844</td>\n",
       "      <td>68.911731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-23</th>\n",
       "      <td>145.740005</td>\n",
       "      <td>145.789993</td>\n",
       "      <td>145.029999</td>\n",
       "      <td>145.300003</td>\n",
       "      <td>116.342499</td>\n",
       "      <td>71966200</td>\n",
       "      <td>145.869995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.237999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279999</td>\n",
       "      <td>55.633764</td>\n",
       "      <td>75.121822</td>\n",
       "      <td>56.969556</td>\n",
       "      <td>0.950990</td>\n",
       "      <td>0.938016</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>-34.674811</td>\n",
       "      <td>1413.043661</td>\n",
       "      <td>20.721379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-26</th>\n",
       "      <td>145.830002</td>\n",
       "      <td>145.949997</td>\n",
       "      <td>144.750000</td>\n",
       "      <td>145.169998</td>\n",
       "      <td>116.238396</td>\n",
       "      <td>69192800</td>\n",
       "      <td>145.300003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145.360999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.229996</td>\n",
       "      <td>35.847984</td>\n",
       "      <td>57.073555</td>\n",
       "      <td>55.143882</td>\n",
       "      <td>0.879746</td>\n",
       "      <td>0.926362</td>\n",
       "      <td>-0.046616</td>\n",
       "      <td>-38.699738</td>\n",
       "      <td>1369.492432</td>\n",
       "      <td>1.828678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-27</th>\n",
       "      <td>143.880005</td>\n",
       "      <td>144.199997</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>139.500000</td>\n",
       "      <td>111.698410</td>\n",
       "      <td>274466500</td>\n",
       "      <td>145.169998</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>144.965999</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.949997</td>\n",
       "      <td>17.671720</td>\n",
       "      <td>36.384489</td>\n",
       "      <td>21.599962</td>\n",
       "      <td>0.361595</td>\n",
       "      <td>0.813409</td>\n",
       "      <td>-0.451814</td>\n",
       "      <td>-93.261454</td>\n",
       "      <td>1256.819371</td>\n",
       "      <td>-292.735006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "Date                                                                     \n",
       "2007-02-21  145.610001  146.070007  145.350006  145.979996  116.886948   \n",
       "2007-02-22  146.050003  146.419998  145.169998  145.869995  116.798935   \n",
       "2007-02-23  145.740005  145.789993  145.029999  145.300003  116.342499   \n",
       "2007-02-26  145.830002  145.949997  144.750000  145.169998  116.238396   \n",
       "2007-02-27  143.880005  144.199997  139.000000  139.500000  111.698410   \n",
       "\n",
       "               volume  prior_close  direction  future_direction         sma  \\\n",
       "Date                                                                          \n",
       "2007-02-21   63971600   146.039993          0                 0  145.144000   \n",
       "2007-02-22   79067400   145.979996          0                 0  145.209999   \n",
       "2007-02-23   71966200   145.869995          0                 0  145.237999   \n",
       "2007-02-26   69192800   145.300003          0                 0  145.360999   \n",
       "2007-02-27  274466500   145.169998          0                 1  144.965999   \n",
       "\n",
       "               ...      momentum     stochk     stochd        rsi      macd  \\\n",
       "Date           ...                                                            \n",
       "2007-02-21     ...      1.089997  89.992783  91.767628  67.286692  1.023571   \n",
       "2007-02-22     ...      0.659988  79.738918  87.692436  65.530479  1.014854   \n",
       "2007-02-23     ...      0.279999  55.633764  75.121822  56.969556  0.950990   \n",
       "2007-02-26     ...      1.229996  35.847984  57.073555  55.143882  0.879746   \n",
       "2007-02-27     ...     -3.949997  17.671720  36.384489  21.599962  0.361595   \n",
       "\n",
       "             macdsig  macdhist  williamsr           ad         cci  \n",
       "Date                                                                \n",
       "2007-02-21  0.914752  0.108819  -7.309015  1437.598145   81.889256  \n",
       "2007-02-22  0.934773  0.080082 -17.027978  1455.101844   68.911731  \n",
       "2007-02-23  0.938016  0.012974 -34.674811  1413.043661   20.721379  \n",
       "2007-02-26  0.926362 -0.046616 -38.699738  1369.492432    1.828678  \n",
       "2007-02-27  0.813409 -0.451814 -93.261454  1256.819371 -292.735006  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = training_set.iloc[33:]  # We discard first rows because some indicators don't have values\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (2142, 10) (2142,)\n",
      "Testing shape: (281, 10) (281,)\n",
      "Validation shape: (270, 10) (270,)\n"
     ]
    }
   ],
   "source": [
    "# macdsig y macdhist no estan en el paper pero igual se auto-calcularon (no se agregan)\n",
    "cols = ['sma', 'wma', 'momentum', 'stochk', 'stochd', 'rsi', 'macd', 'williamsr', 'ad', 'cci']\n",
    "\n",
    "Xtrain = training_set[cols].values\n",
    "Ytrain = training_set['future_direction'].values\n",
    "\n",
    "Xtest = testing_set[cols].values\n",
    "Ytest = testing_set['future_direction'].values\n",
    "\n",
    "Xval = validation_set[cols].values\n",
    "Yval = validation_set['future_direction'].values\n",
    "\n",
    "print('Training shape:', Xtrain.shape, Ytrain.shape)\n",
    "print('Testing shape:', Xtest.shape, Ytest.shape)\n",
    "print('Validation shape:', Xval.shape, Yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data was scaled into the range of `[-1, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain_norm:\n",
      " [[-0.1604 -0.1529  0.4021  0.801   0.8725  0.5052  0.6364  0.8538 -0.9846\n",
      "   0.3832]\n",
      " [-0.1596 -0.1514  0.3841  0.5919  0.7867  0.4577  0.6348  0.6594 -0.9839\n",
      "   0.336 ]\n",
      " [-0.1593 -0.1512  0.3681  0.1004  0.522   0.226   0.6231  0.3065 -0.9856\n",
      "   0.1609]\n",
      " [-0.1579 -0.1513  0.408  -0.3031  0.1419  0.1766  0.6101  0.226  -0.9872\n",
      "   0.0922]\n",
      " [-0.1624 -0.1634  0.1908 -0.6738 -0.2938 -0.7309  0.5156 -0.8652 -0.9916\n",
      "  -0.9782]]\n",
      "Xtest_norm:\n",
      " [[-0.1458 -0.1389  0.1161 -0.5807 -0.5894 -0.1522  0.6403 -0.5165 -0.9941\n",
      "  -0.606 ]\n",
      " [-0.1533 -0.151  -0.2664 -0.6343 -0.6676 -0.682   0.5526 -0.9975 -1.\n",
      "  -0.7918]\n",
      " [-0.1587 -0.1599 -0.1553 -0.7142 -0.7113 -0.5205  0.4949 -0.5754 -0.9953\n",
      "  -0.8519]\n",
      " [-0.1644 -0.1651 -0.171  -0.6551 -0.7378 -0.3121  0.4667 -0.2998 -0.9907\n",
      "  -0.489 ]\n",
      " [-0.1702 -0.1699 -0.1723 -0.399  -0.6542 -0.3488  0.4398 -0.363  -0.9857\n",
      "  -0.4648]]\n",
      "Xval_norm:\n",
      " [[-0.2962 -0.3003 -0.51   -0.6721 -0.6988 -0.6768 -0.4367 -0.9428 -0.991\n",
      "  -0.3954]\n",
      " [-0.2975 -0.3045 -0.2328 -0.399  -0.5703 -0.4866 -0.4394 -0.5703 -0.9887\n",
      "  -0.2966]\n",
      " [-0.2986 -0.299  -0.2165 -0.093  -0.4377 -0.0584 -0.3341  0.4781 -0.9843\n",
      "   0.3298]\n",
      " [-0.2992 -0.2931 -0.1748  0.5112 -0.0225 -0.0543 -0.244   0.8463 -0.9824\n",
      "   0.4976]\n",
      " [-0.2956 -0.284   0.1522  0.8444  0.4138  0.0696 -0.1355  0.736  -0.9822\n",
      "   0.6011]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "Xtrain_norm = scaler.fit_transform(Xtrain)\n",
    "Xtest_norm = scaler.fit_transform(Xtest)\n",
    "Xval_norm = scaler.fit_transform(Xval)\n",
    "\n",
    "print('Xtrain_norm:\\n', Xtrain_norm[:5, :])\n",
    "print('Xtest_norm:\\n', Xtest_norm[:5, :])\n",
    "print('Xval_norm:\\n', Xval_norm[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distribution of Y classes in the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 2142     | Y = 1: 1157 (54.01%)| Y = 0: 985 (45.99%)\n",
      "Test examples: 281       | Y = 1: 153 (54.45%) | Y = 0: 128 (45.55%)\n",
      "Validation examples: 270 | Y = 1: 153 (56.67%) | Y = 0: 117 (43.33%)\n",
      "Total observations: 2693\n"
     ]
    }
   ],
   "source": [
    "Ytrain1 = Ytrain.sum()\n",
    "Ytrain0 = len(Ytrain) - Ytrain1\n",
    "Ytrain1p = Ytrain1 / len(Ytrain) * 100\n",
    "Ytrain0p = Ytrain0 / len(Ytrain) * 100\n",
    "\n",
    "Ytest1 = Ytest.sum()\n",
    "Ytest0 = len(Ytest) - Ytest1\n",
    "Ytest1p = Ytest1 / len(Ytest) * 100\n",
    "Ytest0p = Ytest0 / len(Ytest) * 100\n",
    "\n",
    "Yval1 = Yval.sum()\n",
    "Yval0 = len(Yval) - Yval1\n",
    "Yval1p = Yval1 / len(Yval) * 100\n",
    "Yval0p = Yval0 / len(Yval) * 100\n",
    "\n",
    "print('Train examples: {}     | Y = 1: {} ({:.2f}%)| Y = 0: {} ({:.2f}%)'.format(len(Xtrain_norm), Ytrain1, Ytrain1p, Ytrain0, Ytrain0p))\n",
    "print('Test examples: {}       | Y = 1: {} ({:.2f}%) | Y = 0: {} ({:.2f}%)'.format(len(Xtest_norm), Ytest1, Ytest1p, Ytest0, Ytest0p))\n",
    "print('Validation examples: {} | Y = 1: {} ({:.2f}%) | Y = 0: {} ({:.2f}%)'.format(len(Xval_norm), Yval1, Yval1p, Yval0, Yval0p))\n",
    "print('Total observations:', len(Xtrain) + len(Xtest) + len(Xval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested parameters:\n",
    "- Number of neurons [10, 20, ..., 100]\n",
    "- Epochs (max_iter): [1000, 2000, 10000]\n",
    "- Momentum constant: [0.1, 0.2, ..., 0.9]\n",
    "- Learning rate: 0.1\n",
    "- Hidden layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model (single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=Xtrain_norm.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2142/2142 [==============================] - 0s 109us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 2/500\n",
      "2142/2142 [==============================] - 0s 10us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 3/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 4/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 5/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 6/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 7/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 8/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 9/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 10/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 11/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 12/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 13/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 14/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 15/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 16/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 17/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 18/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 19/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 20/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 21/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 22/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 23/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 24/500\n",
      "2142/2142 [==============================] - 0s 9us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 25/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 26/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 27/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 28/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 29/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 30/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 31/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 32/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 33/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 34/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 35/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 36/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 37/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 38/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 39/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 40/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 41/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 42/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 43/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 44/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 45/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 46/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 47/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 48/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 49/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 50/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 51/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 52/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 53/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 54/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 55/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 56/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 57/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 58/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 59/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 60/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 61/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 62/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 63/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 64/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 65/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 66/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 67/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 68/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 69/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 70/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 71/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 72/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 73/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 74/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 75/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 76/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 77/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 78/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 79/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 80/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 81/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 82/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 83/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 84/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 85/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 86/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 87/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 88/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 89/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 90/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 91/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 92/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 93/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 94/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 95/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 96/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 97/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 98/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 99/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 100/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 101/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 102/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 103/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 104/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 105/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 106/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 107/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 108/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 109/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 110/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 111/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 112/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 113/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 114/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 115/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 116/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 117/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 118/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 119/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 120/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 121/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 122/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 123/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 124/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 125/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 126/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 128/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 129/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 130/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 131/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 132/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 133/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 134/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 135/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 136/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 137/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 138/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 139/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 140/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 141/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 142/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 143/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 144/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 145/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 146/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 147/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 148/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 149/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 150/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 151/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 152/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 153/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 154/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 155/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 156/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 157/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 158/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 159/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 160/500\n",
      "2142/2142 [==============================] - 0s 10us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 161/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 162/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 163/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 164/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 165/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 166/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 167/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 168/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 169/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 170/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 171/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 172/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 173/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 174/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 175/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 176/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 177/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 178/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 179/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 180/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 181/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 182/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 183/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 184/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 185/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 186/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 187/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 188/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 189/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 190/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 191/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 192/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 193/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 194/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 195/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 196/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 197/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 198/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 199/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 200/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 201/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 202/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 203/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 204/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 205/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 206/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 207/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 208/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 209/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 210/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 211/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 212/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 213/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 214/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 215/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 216/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 217/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 218/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 219/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 220/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 221/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 222/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 223/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 224/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 225/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 226/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 227/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 228/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 229/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 230/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 231/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 232/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 233/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 234/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 235/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 236/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 237/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 238/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 239/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 240/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 241/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 242/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 243/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 244/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 245/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 246/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 247/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 248/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 249/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 250/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 251/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 252/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 253/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 254/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 255/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 256/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 257/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 258/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 259/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 260/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 261/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 262/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 263/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 264/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 265/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 266/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 267/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 268/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4969 - acc: 0.5401\n",
      "Epoch 269/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4969 - acc: 0.5401\n",
      "Epoch 270/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 271/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 272/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 273/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 274/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 275/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 276/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 277/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 278/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 279/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 280/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 281/500\n",
      "2142/2142 [==============================] - 0s 9us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 282/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 283/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 284/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 285/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 286/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 287/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 288/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 289/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 290/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 291/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 292/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 293/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 294/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 295/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 296/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 297/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 298/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 299/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 300/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 301/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 302/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 303/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 304/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 305/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 306/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 307/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 308/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 309/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 310/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 311/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 312/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 313/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 314/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 315/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 316/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 317/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 318/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 319/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 320/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 321/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 322/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 323/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 324/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 325/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 326/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 327/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 328/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 329/500\n",
      "2142/2142 [==============================] - 0s 9us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 330/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 331/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 332/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 333/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 334/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 335/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 336/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 337/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 338/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 339/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 340/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 341/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 342/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 343/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 344/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 345/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 346/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 347/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 348/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 349/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 350/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 351/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 352/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 353/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 354/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 355/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 356/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 357/500\n",
      "2142/2142 [==============================] - 0s 11us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 358/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 359/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 360/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 361/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 362/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 363/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 364/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 365/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 366/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 367/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 368/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 369/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 370/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 371/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 372/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 373/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 374/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 375/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 376/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 377/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 378/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 379/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 380/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 381/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 382/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 383/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 384/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 385/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 386/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 387/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 388/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 389/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 390/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 391/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 392/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 393/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 394/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 395/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 396/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 397/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 398/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 399/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 400/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 401/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 402/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 403/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 404/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 405/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 406/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 407/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 408/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 409/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 410/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 411/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 412/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 413/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 414/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 415/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 416/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 417/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 418/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 419/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 420/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 421/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 422/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 423/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 424/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 425/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 426/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 427/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 428/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 429/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4969 - acc: 0.5401\n",
      "Epoch 430/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4969 - acc: 0.5401\n",
      "Epoch 431/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4969 - acc: 0.5401\n",
      "Epoch 432/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 433/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 434/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4968 - acc: 0.5401\n",
      "Epoch 435/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 436/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 437/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 438/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 439/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 440/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 441/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 442/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 443/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 444/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 445/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 446/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 447/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 448/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 449/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 450/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 451/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 452/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 453/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 454/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 455/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 456/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 457/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 458/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 459/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 460/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4963 - acc: 0.5401\n",
      "Epoch 461/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4963 - acc: 0.5401\n",
      "Epoch 462/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4963 - acc: 0.5401\n",
      "Epoch 463/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4963 - acc: 0.5401\n",
      "Epoch 464/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4963 - acc: 0.5401\n",
      "Epoch 465/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 466/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4963 - acc: 0.5401\n",
      "Epoch 467/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 468/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 469/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 470/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4964 - acc: 0.5401\n",
      "Epoch 471/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 472/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 473/500\n",
      "2142/2142 [==============================] - 0s 8us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 474/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 475/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 476/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 477/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4967 - acc: 0.5401\n",
      "Epoch 478/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 479/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 480/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 481/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 482/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 483/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 484/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 485/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 486/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 487/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 488/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4966 - acc: 0.5401\n",
      "Epoch 489/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 490/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 491/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 492/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 493/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 494/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 495/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 496/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 497/500\n",
      "2142/2142 [==============================] - 0s 7us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 498/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 499/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n",
      "Epoch 500/500\n",
      "2142/2142 [==============================] - 0s 6us/step - loss: 0.2482 - mean_absolute_error: 0.4965 - acc: 0.5401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20cc2367390>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mae', 'acc'])\n",
    "model.fit(Xtrain_norm, Ytrain, epochs=500, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 0s 169us/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(Xtest_norm, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 24.84%\n",
      "mean_absolute_error: 49.65%\n",
      "acc: 54.45%\n"
     ]
    }
   ],
   "source": [
    "for i, score in enumerate(scores):\n",
    "    print('{}: {:.2f}%'.format(model.metrics_names[i], score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===== TODO Calculate Metrics with Scikit on Predictions by KERAS ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the model (multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "# 540,000 different parameters to be tested\n",
    "epoch = np.linspace(1000, 10000, 10, dtype=int)\n",
    "neurons = np.linspace(10, 100, 10, dtype=int)\n",
    "momentum = np.linspace(0.1, 0.9, 9)\n",
    "activations = ['logistic', 'tanh', 'relu']\n",
    "solvers = ['sgd', 'adam']\n",
    "lambdas = np.linspace(0.001, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 270 params batches, 2000 each.\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "grid = product(epoch, neurons, momentum, activations, solvers, lambdas)\n",
    "grid_list = list(grid)\n",
    "param_buckets = []\n",
    "for r in range(0, len(grid_list), 2000):\n",
    "    param_buckets.append(grid_list[r:r + 2000])\n",
    "    \n",
    "print('Generated', len(param_buckets), 'params batches, 2000 each.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comparative = {}\n",
    "# Took 3 min to measure a single bucket (2000 params)\n",
    "\n",
    "for params in param_buckets[0]:\n",
    "    ep, n, mc, activation, solver, lr = params\n",
    "    \n",
    "    mlp.set_params(\n",
    "        max_iter=ep,\n",
    "        hidden_layer_sizes=(n, n, n),\n",
    "        momentum=mc,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        learning_rate_init=lr)\n",
    "    \n",
    "    mlp.fit(Xtrain_norm, Ytrain)\n",
    "    comparative[params] = mlp.score(Xtest_norm, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell takes to much time!!\n",
    "from itertools import product\n",
    "comparative = {}\n",
    "\n",
    "#for params in product(epoch, neurons, momentum, activations, solvers, lambdas):\n",
    "    ep, n, mc, activation, solver, lr = params\n",
    "    \n",
    "    mlp.set_params(\n",
    "        max_iter=ep,\n",
    "        hidden_layer_sizes=(n, n, n),\n",
    "        momentum=mc,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        learning_rate_init=lr)\n",
    "    \n",
    "    mlp.fit(Xtrain_norm, Ytrain)\n",
    "    comparative[params] = mlp.score(Xtest_norm, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest score in the 540,000 parameter combinations: 42.70%\n",
      "Max score in the 540,000 parameter combinations: 57.30%\n"
     ]
    }
   ],
   "source": [
    "print('Lowest score in the 540,000 parameter combinations: {:.2f}%'.format(min(comparative.values()) * 100))\n",
    "print('Max score in the 540,000 parameter combinations: {:.2f}%'.format(max(comparative.values()) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000  |  Neurons: 10  |  MC: 0.2  |  Activation: relu  |  Solver: adam  |  LR:  0.078  |  Score: 57.2954%\n",
      "Epoch: 1000  |  Neurons: 10  |  MC: 0.1  |  Activation: tanh  |  Solver: adam  |  LR:  0.085  |  Score: 56.9395%\n",
      "Epoch: 1000  |  Neurons: 10  |  MC: 0.2  |  Activation: tanh  |  Solver: sgd  |  LR:  0.019  |  Score: 56.9395%\n",
      "Epoch: 1000  |  Neurons: 10  |  MC: 0.1  |  Activation: tanh  |  Solver: sgd  |  LR:  0.010  |  Score: 56.5836%\n",
      "Epoch: 1000  |  Neurons: 10  |  MC: 0.1  |  Activation: tanh  |  Solver: sgd  |  LR:  0.093  |  Score: 56.5836%\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "key_params = heapq.nlargest(5, comparative, key=comparative.get) # Get top 5 by best score\n",
    "\n",
    "for key in key_params:\n",
    "    print('Epoch: {}  |  Neurons: {}  |  MC: {:.1f}  |  Activation: {}  |  Solver: {}  |  LR:  {:.3f}  |  Score: {:.4f}%'.format(\n",
    "        *key, comparative[key] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing performance in sets of best score parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating yearly scores for the best 1st set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ep, n, mc, activation, solver, lr = key_params[0]  # 1st set of best parameters\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    learning_rate_init=lr,\n",
    "    activation=activation,\n",
    "    max_iter=ep,\n",
    "    hidden_layer_sizes=(n, n, n),\n",
    "    momentum=mc,\n",
    "    solver=solver)\n",
    "\n",
    "mlp.fit(Xtrain_norm, Ytrain)\n",
    "Ypred_train = mlp.predict(Xtrain_norm)  # First analysis predicts same training set\n",
    "Ypred_test = mlp.predict(Xtest_norm)  # Second analysis predicts testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classification = pd.DataFrame({'Ytrain': Ytrain, 'Ypred_train': Ypred_train}, index=training_set.index)\n",
    "train_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_classification = pd.DataFrame({'Ytest': Ytest, 'Ypred_test': Ypred_test}, index=testing_set.index)\n",
    "test_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_benchmark = train_classification['Ytrain'] == train_classification['Ypred_train']\n",
    "test_benchmark = test_classification['Ytest'] == test_classification['Ypred_test']\n",
    "\n",
    "train_score = train_benchmark.groupby(train_benchmark.index.year).apply(lambda s: s[s == True].count() / s.count()).rename('train_score')\n",
    "test_score = test_benchmark.groupby(test_benchmark.index.year).apply(lambda s: s[s == True].count() / s.count()).rename('test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_one = pd.concat([train_score, test_score], axis=1)\n",
    "benchmark_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train average: {:.2f}\\nTest average: {:.2f}'.format(\n",
    "    benchmark_one['train_score'].mean() * 100,\n",
    "    benchmark_one['test_score'].mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating yearly scores for the best 2nd set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ep, n, mc, activation, solver, lr = key_params[1]  # 2nd set of best parameters\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    learning_rate_init=lr,\n",
    "    activation=activation,\n",
    "    max_iter=ep,\n",
    "    hidden_layer_sizes=(n, n, n),\n",
    "    momentum=mc,\n",
    "    solver=solver)\n",
    "\n",
    "mlp.fit(Xtrain_norm, Ytrain)\n",
    "Ypred_train = mlp.predict(Xtrain_norm)  # First analysis predicts same training set\n",
    "Ypred_test = mlp.predict(Xtest_norm)  # Second analysis predicts testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classification = pd.DataFrame({'Ytrain': Ytrain, 'Ypred_train': Ypred_train}, index=training_set.index)\n",
    "test_classification = pd.DataFrame({'Ytest': Ytest, 'Ypred_test': Ypred_test}, index=testing_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_benchmark = train_classification['Ytrain'] == train_classification['Ypred_train']\n",
    "test_benchmark = test_classification['Ytest'] == test_classification['Ypred_test']\n",
    "\n",
    "train_score = train_benchmark.groupby(train_benchmark.index.year).apply(lambda s: s[s == True].count() / s.count()).rename('train_score')\n",
    "test_score = test_benchmark.groupby(test_benchmark.index.year).apply(lambda s: s[s == True].count() / s.count()).rename('test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_two = pd.concat([train_score, test_score], axis=1)\n",
    "benchmark_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train average: {:.2f}\\nTest average: {:.2f}'.format(\n",
    "    benchmark_two['train_score'].mean() * 100,\n",
    "    benchmark_two['test_score'].mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating yearly scores for the best 3rd set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ep, n, mc, activation, solver, lr = key_params[2]  # 3rd set of best parameters\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    learning_rate_init=lr,\n",
    "    activation=activation,\n",
    "    max_iter=ep,\n",
    "    hidden_layer_sizes=(n, n, n),\n",
    "    momentum=mc,\n",
    "    solver=solver)\n",
    "\n",
    "mlp.fit(Xtrain_norm, Ytrain)\n",
    "Ypred_train = mlp.predict(Xtrain_norm)  # First analysis predicts same training set\n",
    "Ypred_test = mlp.predict(Xtest_norm)  # Second analysis predicts testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classification = pd.DataFrame({'Ytrain': Ytrain, 'Ypred_train': Ypred_train}, index=training_set.index)\n",
    "test_classification = pd.DataFrame({'Ytest': Ytest, 'Ypred_test': Ypred_test}, index=testing_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_benchmark = train_classification['Ytrain'] == train_classification['Ypred_train']\n",
    "test_benchmark = test_classification['Ytest'] == test_classification['Ypred_test']\n",
    "\n",
    "train_score = train_benchmark.groupby(train_benchmark.index.year).apply(lambda s: s[s == True].count() / s.count()).rename('train_score')\n",
    "test_score = test_benchmark.groupby(test_benchmark.index.year).apply(lambda s: s[s == True].count() / s.count()).rename('test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_three = pd.concat([train_score, test_score], axis=1)\n",
    "benchmark_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train average: {:.2f}%\\nTest average: {:.2f}%'.format(\n",
    "    benchmark_three['train_score'].mean() * 100,\n",
    "    benchmark_three['test_score'].mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Later: Also test various values of learning_rate, hidden_layers and activation function.\n",
    "# Later: train the network X times for each parameters to have statistical significance\n",
    "# Later: train the network with multiple market data to see how it behaves in other markets\n",
    "# Later: Backtest a strategy using the prediction results to get a set of trade results\n",
    "# Later: Analyze trade results with CDF and Montecarlo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
